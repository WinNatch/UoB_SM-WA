{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "R63OirZ6nkb7",
    "outputId": "02496c03-f6b8-448b-9fb3-5f52cea18002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date   Time                                           Headline  \\\n",
      "0  3/17/2025  15:14  Is Salah's cup finals record his Achilles heel...   \n",
      "1  3/17/2025  15:00  Liverpool 'can suffer' final defeat - Jamespub...   \n",
      "2  3/17/2025  13:55  'We want to bounce back, and we will'published...   \n",
      "3  3/17/2025  12:06  'Lack of quality depth must be addressed'publi...   \n",
      "4  3/17/2025  09:55  'We played into their hands'published at 09:55...   \n",
      "\n",
      "                                        Body Context  \n",
      "0  Alongside his team-mates, it was a difficult d...  \n",
      "1  Former Liverpool goalkeeper David James says L...  \n",
      "2  Liverpool captain Virgil van Dijk on Instagram...  \n",
      "3  In the space of a week, the Arne Slot machine ...  \n",
      "4  Andy Robertson says Liverpool \"didn't use the ...  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "edge_driver_path = r\"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedgedriver.exe\"\n",
    "edge_options = EdgeOptions()\n",
    "edge_options.add_argument(\"--headless\")\n",
    "service = EdgeService(edge_driver_path)\n",
    "driver = webdriver.Edge(service=service, options=edge_options)\n",
    "\n",
    "url = \"https://www.bbc.co.uk/sport/football/teams/liverpool\"\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(5)\n",
    "html = driver.page_source\n",
    "driver.quit()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "articles = soup.find_all(\"article\", class_=\"ssrcss-1epmku9-ContentPost e6wdqbx1\")\n",
    "\n",
    "news_data = []\n",
    "\n",
    "for article in articles:\n",
    "    date_span = article.find(\"span\")\n",
    "    if date_span:\n",
    "        raw_date = date_span.get_text(separator=\" \", strip=True)\n",
    "        \n",
    "        time_matches = re.findall(r\"(\\d{1,2}:\\d{2})\", raw_date)\n",
    "        if time_matches:\n",
    "            time_text = time_matches[0]\n",
    "        else:\n",
    "            time_text = \"00:00\"\n",
    "        \n",
    "        date_part = re.sub(r\"\\d{1,2}:\\d{2}\", \"\", raw_date).strip()\n",
    "        tokens = date_part.split()\n",
    "        if len(tokens) >= 2:\n",
    "            extracted_date_str = \" \".join(tokens[:2])\n",
    "            try:\n",
    "                date_obj = datetime.strptime(extracted_date_str, \"%d %B\")\n",
    "                date_text = date_obj.replace(year=2025).strftime(\"%#m/%#d/%Y\")\n",
    "            except Exception as e:\n",
    "                date_text = \"3/17/2025\"\n",
    "        else:\n",
    "            date_text = \"3/17/2025\"\n",
    "    else:\n",
    "        date_text = \"3/17/2025\"\n",
    "        time_text = \"00:00\"\n",
    "    \n",
    "    headline_tag = article.find(\"span\", class_=\"ssrcss-189b1h2-HeadlineWrap e14e9ror2\")\n",
    "    if headline_tag:\n",
    "        headline_text = headline_tag.get_text(strip=True)\n",
    "    else:\n",
    "        headline_text = \"No Headline\"\n",
    "    \n",
    "    paragraphs = article.find_all(\"p\", class_=re.compile(\"Paragraph\"))\n",
    "    if paragraphs:\n",
    "        body_text = \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "    else:\n",
    "        body_text = \"No Content\"\n",
    "    \n",
    "    news_data.append([date_text, time_text, headline_text, body_text])\n",
    "\n",
    "df = pd.DataFrame(news_data, columns=[\"Date\", \"Time\", \"Headline\", \"Body Context\"])\n",
    "print(df.head())\n",
    "df.to_csv(\"Test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1: https://www.bbc.co.uk/sport/football/teams/liverpool\n",
      "Scraping Page 2: https://www.bbc.co.uk/sport/football/teams/liverpool?page=2\n",
      "Scraping Page 3: https://www.bbc.co.uk/sport/football/teams/liverpool?page=3\n",
      "Scraping Page 4: https://www.bbc.co.uk/sport/football/teams/liverpool?page=4\n",
      "Scraping Page 5: https://www.bbc.co.uk/sport/football/teams/liverpool?page=5\n",
      "Scraping Page 6: https://www.bbc.co.uk/sport/football/teams/liverpool?page=6\n",
      "Scraping Page 7: https://www.bbc.co.uk/sport/football/teams/liverpool?page=7\n",
      "Scraping Page 8: https://www.bbc.co.uk/sport/football/teams/liverpool?page=8\n",
      "Scraping Page 9: https://www.bbc.co.uk/sport/football/teams/liverpool?page=9\n",
      "Scraping Page 10: https://www.bbc.co.uk/sport/football/teams/liverpool?page=10\n",
      "Scraping Page 11: https://www.bbc.co.uk/sport/football/teams/liverpool?page=11\n",
      "Scraping Page 12: https://www.bbc.co.uk/sport/football/teams/liverpool?page=12\n",
      "Scraping Page 13: https://www.bbc.co.uk/sport/football/teams/liverpool?page=13\n",
      "Scraping Page 14: https://www.bbc.co.uk/sport/football/teams/liverpool?page=14\n",
      "Scraping Page 15: https://www.bbc.co.uk/sport/football/teams/liverpool?page=15\n",
      "Scraping Page 16: https://www.bbc.co.uk/sport/football/teams/liverpool?page=16\n",
      "Scraping Page 17: https://www.bbc.co.uk/sport/football/teams/liverpool?page=17\n",
      "Scraping Page 18: https://www.bbc.co.uk/sport/football/teams/liverpool?page=18\n",
      "Scraping Page 19: https://www.bbc.co.uk/sport/football/teams/liverpool?page=19\n",
      "Scraping Page 20: https://www.bbc.co.uk/sport/football/teams/liverpool?page=20\n",
      "Scraping Page 21: https://www.bbc.co.uk/sport/football/teams/liverpool?page=21\n",
      "Scraping Page 22: https://www.bbc.co.uk/sport/football/teams/liverpool?page=22\n",
      "Scraping Page 23: https://www.bbc.co.uk/sport/football/teams/liverpool?page=23\n",
      "Scraping Page 24: https://www.bbc.co.uk/sport/football/teams/liverpool?page=24\n",
      "Scraping Page 25: https://www.bbc.co.uk/sport/football/teams/liverpool?page=25\n",
      "Scraping Page 26: https://www.bbc.co.uk/sport/football/teams/liverpool?page=26\n",
      "Scraping Page 27: https://www.bbc.co.uk/sport/football/teams/liverpool?page=27\n",
      "Scraping Page 28: https://www.bbc.co.uk/sport/football/teams/liverpool?page=28\n",
      "Scraping Page 29: https://www.bbc.co.uk/sport/football/teams/liverpool?page=29\n",
      "Scraping Page 30: https://www.bbc.co.uk/sport/football/teams/liverpool?page=30\n",
      "Scraping Page 31: https://www.bbc.co.uk/sport/football/teams/liverpool?page=31\n",
      "Scraping Page 32: https://www.bbc.co.uk/sport/football/teams/liverpool?page=32\n",
      "Scraping Page 33: https://www.bbc.co.uk/sport/football/teams/liverpool?page=33\n",
      "Scraping Page 34: https://www.bbc.co.uk/sport/football/teams/liverpool?page=34\n",
      "Scraping Page 35: https://www.bbc.co.uk/sport/football/teams/liverpool?page=35\n",
      "Scraping Page 36: https://www.bbc.co.uk/sport/football/teams/liverpool?page=36\n",
      "Scraping Page 37: https://www.bbc.co.uk/sport/football/teams/liverpool?page=37\n",
      "Scraping Page 38: https://www.bbc.co.uk/sport/football/teams/liverpool?page=38\n",
      "Scraping Page 39: https://www.bbc.co.uk/sport/football/teams/liverpool?page=39\n",
      "Scraping Page 40: https://www.bbc.co.uk/sport/football/teams/liverpool?page=40\n",
      "Scraping complete! Data saved to AllPages.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "edge_driver_path = r\"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedgedriver.exe\"\n",
    "edge_options = EdgeOptions()\n",
    "edge_options.add_argument(\"--headless\")\n",
    "service = EdgeService(edge_driver_path)\n",
    "driver = webdriver.Edge(service=service, options=edge_options)\n",
    "\n",
    "all_news_data = [] \n",
    "\n",
    "for page in range(1, 41):\n",
    "    if page == 1:\n",
    "        url = \"https://www.bbc.co.uk/sport/football/teams/liverpool\"\n",
    "    else:\n",
    "        url = f\"https://www.bbc.co.uk/sport/football/teams/liverpool?page={page}\"\n",
    "\n",
    "    print(f\"Scraping Page {page}: {url}\")\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    html = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"ssrcss-1epmku9-ContentPost e6wdqbx1\")\n",
    "\n",
    "    for article in articles:\n",
    "        date_span = article.find(\"span\")\n",
    "        if date_span:\n",
    "            raw_date = date_span.get_text(separator=\" \", strip=True)\n",
    "            \n",
    "            time_matches = re.findall(r\"(\\d{1,2}:\\d{2})\", raw_date)\n",
    "            if time_matches:\n",
    "                time_text = time_matches[0]\n",
    "            else:\n",
    "                time_text = \"00:00\"\n",
    "            \n",
    "            date_part = re.sub(r\"\\d{1,2}:\\d{2}\", \"\", raw_date).strip()\n",
    "            tokens = date_part.split()\n",
    "            if len(tokens) >= 2:\n",
    "                extracted_date_str = \" \".join(tokens[:2])\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(extracted_date_str, \"%d %B\")\n",
    "                    date_text = date_obj.replace(year=2025).strftime(\"%#m/%#d/%Y\")\n",
    "                except Exception:\n",
    "                    date_text = \"3/17/2025\"\n",
    "            else:\n",
    "                date_text = \"3/17/2025\"\n",
    "        else:\n",
    "            date_text = \"3/17/2025\"\n",
    "            time_text = \"00:00\"\n",
    "        \n",
    "        headline_tag = article.find(\"span\", class_=\"ssrcss-189b1h2-HeadlineWrap e14e9ror2\")\n",
    "        if headline_tag:\n",
    "            headline_text = headline_tag.get_text(strip=True)\n",
    "            lower_headline = headline_text.lower()\n",
    "            idx = lower_headline.find(\"published at\")\n",
    "            if idx != -1:\n",
    "                headline_text = headline_text[:idx].strip()\n",
    "        else:\n",
    "            headline_text = \"No Headline\"\n",
    "        \n",
    "        paragraphs = article.find_all(\"p\", class_=re.compile(\"Paragraph\"))\n",
    "        if paragraphs:\n",
    "            body_text = \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "        else:\n",
    "            body_text = \"No Content\"\n",
    "        \n",
    "        all_news_data.append([date_text, time_text, headline_text, body_text])\n",
    "\n",
    "driver.quit()\n",
    "df = pd.DataFrame(all_news_data, columns=[\"Date\", \"Time\", \"Headline\", \"Body Context\"])\n",
    "df.to_csv(\"BBC_AllPages.csv\", index=False)\n",
    "print(\"Scraping complete! Data saved to AllPages.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
